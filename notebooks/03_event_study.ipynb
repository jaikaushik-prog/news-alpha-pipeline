{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03 - Event Study Analysis\n",
                "\n",
                "This notebook performs the core event study analysis:\n",
                "1. Load processed speech data\n",
                "2. Build sector portfolios\n",
                "3. Detect first sector mentions\n",
                "4. Calculate Cumulative Abnormal Returns (CAR)\n",
                "5. Run statistical tests\n",
                "6. Visualize results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "AttributeError",
                    "evalue": "partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m project_root \u001b[38;5;241m=\u001b[39m Path\u001b[38;5;241m.\u001b[39mcwd()\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m      7\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(project_root))\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     ArrowDtype,\n\u001b[0;32m     52\u001b[0m     Int8Dtype,\n\u001b[0;32m     53\u001b[0m     Int16Dtype,\n\u001b[0;32m     54\u001b[0m     Int32Dtype,\n\u001b[0;32m     55\u001b[0m     Int64Dtype,\n\u001b[0;32m     56\u001b[0m     UInt8Dtype,\n\u001b[0;32m     57\u001b[0m     UInt16Dtype,\n\u001b[0;32m     58\u001b[0m     UInt32Dtype,\n\u001b[0;32m     59\u001b[0m     UInt64Dtype,\n\u001b[0;32m     60\u001b[0m     Float32Dtype,\n\u001b[0;32m     61\u001b[0m     Float64Dtype,\n\u001b[0;32m     62\u001b[0m     CategoricalDtype,\n\u001b[0;32m     63\u001b[0m     PeriodDtype,\n\u001b[0;32m     64\u001b[0m     IntervalDtype,\n\u001b[0;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     66\u001b[0m     StringDtype,\n\u001b[0;32m     67\u001b[0m     BooleanDtype,\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     NA,\n\u001b[0;32m     70\u001b[0m     isna,\n\u001b[0;32m     71\u001b[0m     isnull,\n\u001b[0;32m     72\u001b[0m     notna,\n\u001b[0;32m     73\u001b[0m     notnull,\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     Index,\n\u001b[0;32m     76\u001b[0m     CategoricalIndex,\n\u001b[0;32m     77\u001b[0m     RangeIndex,\n\u001b[0;32m     78\u001b[0m     MultiIndex,\n\u001b[0;32m     79\u001b[0m     IntervalIndex,\n\u001b[0;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     81\u001b[0m     DatetimeIndex,\n\u001b[0;32m     82\u001b[0m     PeriodIndex,\n\u001b[0;32m     83\u001b[0m     IndexSlice,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     NaT,\n\u001b[0;32m     86\u001b[0m     Period,\n\u001b[0;32m     87\u001b[0m     period_range,\n\u001b[0;32m     88\u001b[0m     Timedelta,\n\u001b[0;32m     89\u001b[0m     timedelta_range,\n\u001b[0;32m     90\u001b[0m     Timestamp,\n\u001b[0;32m     91\u001b[0m     date_range,\n\u001b[0;32m     92\u001b[0m     bdate_range,\n\u001b[0;32m     93\u001b[0m     Interval,\n\u001b[0;32m     94\u001b[0m     interval_range,\n\u001b[0;32m     95\u001b[0m     DateOffset,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     to_numeric,\n\u001b[0;32m     98\u001b[0m     to_datetime,\n\u001b[0;32m     99\u001b[0m     to_timedelta,\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     Flags,\n\u001b[0;32m    102\u001b[0m     Grouper,\n\u001b[0;32m    103\u001b[0m     factorize,\n\u001b[0;32m    104\u001b[0m     unique,\n\u001b[0;32m    105\u001b[0m     value_counts,\n\u001b[0;32m    106\u001b[0m     NamedAgg,\n\u001b[0;32m    107\u001b[0m     array,\n\u001b[0;32m    108\u001b[0m     Categorical,\n\u001b[0;32m    109\u001b[0m     set_eng_float_format,\n\u001b[0;32m    110\u001b[0m     Series,\n\u001b[0;32m    111\u001b[0m     DataFrame,\n\u001b[0;32m    112\u001b[0m )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
                        "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
                        "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
                        "File \u001b[1;32minterval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
                        "File \u001b[1;32mhashtable.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.hashtable\u001b[1;34m()\u001b[0m\n",
                        "File \u001b[1;32mmissing.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.missing\u001b[1;34m()\u001b[0m\n",
                        "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\tslibs\\__init__.py:39\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtypes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalize_pydatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_supported_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     37\u001b[0m ]\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes  \u001b[38;5;66;03m# pylint: disable=import-self\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m localize_pydatetime\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     42\u001b[0m     Resolution,\n\u001b[0;32m     43\u001b[0m     periods_per_day,\n\u001b[0;32m     44\u001b[0m     periods_per_second,\n\u001b[0;32m     45\u001b[0m )\n",
                        "File \u001b[1;32mdtypes.pyx:155\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.dtypes\u001b[1;34m()\u001b[0m\n",
                        "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)"
                    ]
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
                        "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
                        "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "# Setup\n",
                "\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "project_root = Path.cwd().parent\n",
                "sys.path.insert(0, str(project_root))\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from datetime import datetime, timedelta\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "print(f\"Project root: {project_root}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Processed Speech Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load processed sentences\n",
                "speech_dir = project_root / 'data' / 'intermediate' / 'speech_text'\n",
                "\n",
                "if speech_dir.exists():\n",
                "    speech_files = list(speech_dir.glob('*.parquet'))\n",
                "    print(f\"Found {len(speech_files)} processed speech files\")\n",
                "    \n",
                "    sentences_by_year = {}\n",
                "    for f in speech_files:\n",
                "        fy = f.stem.replace('_sentences', '').replace('_', '-')\n",
                "        sentences_by_year[fy] = pd.read_parquet(f)\n",
                "        print(f\"  {fy}: {len(sentences_by_year[fy])} sentences\")\n",
                "else:\n",
                "    print(\"No processed speech data found. Run 02_nlp_processing.ipynb first.\")\n",
                "    # Demo mode - process one speech\n",
                "    print(\"\\nRunning demo processing...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# If no processed data, run quick processing\n",
                "if not sentences_by_year:\n",
                "    from src.nlp import extract_text_with_fallback, clean_text, tokenize_speech, estimate_timestamps\n",
                "    from src.nlp import classify_sectors_batch, analyze_sentiment_batch, score_certainty_batch\n",
                "    import yaml\n",
                "    \n",
                "    # Load config\n",
                "    with open(project_root / 'config' / 'sectors.yaml', 'r', encoding='utf-8') as f:\n",
                "        sectors_config = yaml.safe_load(f)\n",
                "    \n",
                "    # Find latest speech\n",
                "    speech_file = list(project_root.glob('*2024*.pdf'))[0]\n",
                "    print(f\"Processing {speech_file.name}...\")\n",
                "    \n",
                "    # Quick processing\n",
                "    text = clean_text(extract_text_with_fallback(str(speech_file)))\n",
                "    sentences = tokenize_speech(text)\n",
                "    sentences_df = pd.DataFrame(sentences)\n",
                "    \n",
                "    # Classify sectors\n",
                "    sector_keywords = {k: v.get('keywords', []) for k, v in sectors_config['sectors'].items()}\n",
                "    prob_cols = classify_sectors_batch(sentences_df['text'].tolist(), sector_keywords)\n",
                "    for col, probs in prob_cols.items():\n",
                "        sentences_df[col] = probs\n",
                "    \n",
                "    sentences_df['importance_weight'] = 1.0\n",
                "    sentences_by_year = {'2024-25': sentences_df}\n",
                "    print(f\"Processed {len(sentences_df)} sentences\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Detect First Sector Mentions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.events import detect_all_sector_mentions, calculate_cumulative_attention\n",
                "\n",
                "# Detect mentions for each year\n",
                "mentions_by_year = {}\n",
                "\n",
                "for fy, sentences_df in sentences_by_year.items():\n",
                "    mentions = detect_all_sector_mentions(sentences_df, threshold=0.3)\n",
                "    mentions_by_year[fy] = mentions\n",
                "    \n",
                "    print(f\"\\n{fy}: Detected {len(mentions)} sector first mentions\")\n",
                "    if not mentions.empty:\n",
                "        print(mentions[['sector', 'sentence_position', 'cumulative_attention']].to_string())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize mention order\n",
                "if mentions_by_year:\n",
                "    # Take one year for visualization\n",
                "    fy = list(mentions_by_year.keys())[0]\n",
                "    mentions = mentions_by_year[fy]\n",
                "    sentences_df = sentences_by_year[fy]\n",
                "    \n",
                "    if not mentions.empty:\n",
                "        fig, ax = plt.subplots(figsize=(14, 6))\n",
                "        \n",
                "        # Plot cumulative attention for each sector\n",
                "        prob_cols = [c for c in sentences_df.columns if c.startswith('prob_')]\n",
                "        \n",
                "        for col in prob_cols[:8]:  # Top 8 sectors\n",
                "            sector = col.replace('prob_', '')\n",
                "            cum_attn = calculate_cumulative_attention(sentences_df, sector)\n",
                "            ax.plot(sentences_df['position'], cum_attn, label=sector, alpha=0.7)\n",
                "        \n",
                "        ax.axhline(y=0.3, color='red', linestyle='--', alpha=0.5, label='Mention Threshold')\n",
                "        ax.set_xlabel('Sentence Position')\n",
                "        ax.set_ylabel('Cumulative Attention')\n",
                "        ax.set_title(f'Cumulative Sector Attention - Budget {fy}')\n",
                "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
                "        plt.tight_layout()\n",
                "        plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Market Data for Budget Day"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.ingestion import load_single_stock, get_budget_dates\n",
                "from src.market import clean_stock_data\n",
                "import yaml\n",
                "\n",
                "# Load config\n",
                "with open(project_root / 'config' / 'sectors.yaml', 'r', encoding='utf-8') as f:\n",
                "    sectors_config = yaml.safe_load(f)\n",
                "\n",
                "with open(project_root / 'config' / 'event_dates.yaml', 'r', encoding='utf-8') as f:\n",
                "    event_dates = yaml.safe_load(f)\n",
                "\n",
                "# Get budget date for 2024-25\n",
                "budget_info = event_dates['budget_events'].get('2024-25', {})\n",
                "budget_date = datetime.strptime(budget_info.get('date', '2024-07-23'), '%Y-%m-%d').date()\n",
                "print(f\"Loading market data for: {budget_date}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load stock data for key sectors\n",
                "sample_sectors = ['banking_nbfc', 'infrastructure', 'it_services', 'pharma', 'energy_power']\n",
                "\n",
                "sector_data = {}\n",
                "\n",
                "for sector_key in sample_sectors:\n",
                "    sector_info = sectors_config['sectors'].get(sector_key, {})\n",
                "    stocks = sector_info.get('stocks', [])[:10]  # First 10 stocks per sector\n",
                "    \n",
                "    stock_returns = []\n",
                "    \n",
                "    for symbol in stocks:\n",
                "        df = load_single_stock(symbol)\n",
                "        if df.empty:\n",
                "            continue\n",
                "        \n",
                "        # Filter to budget day\n",
                "        df = df[df.index.date == budget_date]\n",
                "        \n",
                "        if len(df) > 0:\n",
                "            df = clean_stock_data(df)\n",
                "            if 'return' in df.columns:\n",
                "                stock_returns.append(df['return'].rename(symbol))\n",
                "    \n",
                "    if stock_returns:\n",
                "        sector_df = pd.concat(stock_returns, axis=1)\n",
                "        sector_df['portfolio_return'] = sector_df.mean(axis=1)\n",
                "        sector_data[sector_key] = sector_df\n",
                "        print(f\"{sector_key}: {len(stock_returns)} stocks, {len(sector_df)} bars\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combine sector returns\n",
                "if sector_data:\n",
                "    sector_returns = pd.DataFrame({\n",
                "        sector: data['portfolio_return'] for sector, data in sector_data.items()\n",
                "    })\n",
                "    \n",
                "    print(f\"Sector returns shape: {sector_returns.shape}\")\n",
                "    print(f\"Time range: {sector_returns.index.min()} to {sector_returns.index.max()}\")\n",
                "    sector_returns.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Calculate Abnormal Returns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models import calculate_abnormal_returns\n",
                "\n",
                "if 'sector_returns' in dir() and not sector_returns.empty:\n",
                "    # Market return = average across sectors\n",
                "    market_return = sector_returns.mean(axis=1)\n",
                "    \n",
                "    # Calculate abnormal returns\n",
                "    abnormal_returns = calculate_abnormal_returns(sector_returns, market_return, method='market_adjusted')\n",
                "    \n",
                "    print(\"Abnormal returns statistics:\")\n",
                "    print(abnormal_returns.describe())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize cumulative returns around budget speech\n",
                "if 'sector_returns' in dir() and not sector_returns.empty:\n",
                "    fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
                "    \n",
                "    # Raw cumulative returns\n",
                "    ax1 = axes[0]\n",
                "    for col in sector_returns.columns:\n",
                "        cum_ret = (1 + sector_returns[col]).cumprod() - 1\n",
                "        ax1.plot(sector_returns.index, cum_ret * 100, label=col, linewidth=1.5)\n",
                "    \n",
                "    # Mark speech start (11:00)\n",
                "    from src.utils.time_utils import IST\n",
                "    speech_start = datetime.combine(budget_date, datetime.strptime('11:00', '%H:%M').time())\n",
                "    speech_start = IST.localize(speech_start)\n",
                "    ax1.axvline(x=speech_start, color='red', linestyle='--', alpha=0.7, label='Speech Start')\n",
                "    \n",
                "    ax1.set_ylabel('Cumulative Return (%)')\n",
                "    ax1.set_title(f'Sector Returns on Budget Day {budget_date}')\n",
                "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
                "    ax1.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
                "    \n",
                "    # Cumulative Abnormal Returns\n",
                "    ax2 = axes[1]\n",
                "    for col in abnormal_returns.columns:\n",
                "        car = abnormal_returns[col].cumsum()\n",
                "        ax2.plot(abnormal_returns.index, car * 100, label=col, linewidth=1.5)\n",
                "    \n",
                "    ax2.axvline(x=speech_start, color='red', linestyle='--', alpha=0.7)\n",
                "    ax2.set_xlabel('Time (IST)')\n",
                "    ax2.set_ylabel('Cumulative Abnormal Return (%)')\n",
                "    ax2.set_title('Cumulative Abnormal Returns')\n",
                "    ax2.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Event Study Around First Mentions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models import event_study_single\n",
                "\n",
                "# Run event study for each sector mention\n",
                "if 'abnormal_returns' in dir() and mentions_by_year:\n",
                "    fy = list(mentions_by_year.keys())[0]\n",
                "    mentions = mentions_by_year[fy]\n",
                "    \n",
                "    event_results = []\n",
                "    \n",
                "    for _, mention in mentions.iterrows():\n",
                "        sector = mention['sector']\n",
                "        \n",
                "        # Use estimated timestamp or position-based\n",
                "        if 'estimated_timestamp' in mention and pd.notna(mention['estimated_timestamp']):\n",
                "            event_time = mention['estimated_timestamp']\n",
                "        else:\n",
                "            # Estimate based on position\n",
                "            position = mention.get('sentence_position', 0)\n",
                "            total_sentences = len(sentences_by_year[fy])\n",
                "            progress = position / max(total_sentences, 1)\n",
                "            event_time = speech_start + timedelta(minutes=progress * 90)\n",
                "        \n",
                "        if sector in abnormal_returns.columns:\n",
                "            result = event_study_single(abnormal_returns, event_time, sector)\n",
                "            result['fiscal_year'] = fy\n",
                "            event_results.append(result)\n",
                "    \n",
                "    event_df = pd.DataFrame(event_results)\n",
                "    print(f\"Event study results: {len(event_df)} events\")\n",
                "    if not event_df.empty:\n",
                "        print(event_df[['sector', 'car_5m', 'car_15m', 'car_30m', 'car_60m']].to_string())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Statistical Significance Tests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy import stats\n",
                "\n",
                "if 'event_df' in dir() and not event_df.empty:\n",
                "    print(\"CAR Significance Tests:\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    for col in ['car_5m', 'car_15m', 'car_30m', 'car_60m']:\n",
                "        if col in event_df.columns:\n",
                "            values = event_df[col].dropna()\n",
                "            if len(values) > 1:\n",
                "                mean = values.mean()\n",
                "                std = values.std()\n",
                "                n = len(values)\n",
                "                t_stat = mean / (std / np.sqrt(n)) if std > 0 else 0\n",
                "                p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n-1)) if n > 1 else 1\n",
                "                \n",
                "                print(f\"\\n{col}:\")\n",
                "                print(f\"  Mean CAR: {mean*100:.3f}%\")\n",
                "                print(f\"  Std Dev: {std*100:.3f}%\")\n",
                "                print(f\"  t-stat: {t_stat:.3f}\")\n",
                "                print(f\"  p-value: {p_value:.4f}\")\n",
                "                print(f\"  Significant at 5%: {'Yes' if p_value < 0.05 else 'No'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary and Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results\n",
                "output_dir = project_root / 'outputs' / 'tables'\n",
                "output_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "if 'event_df' in dir() and not event_df.empty:\n",
                "    event_df.to_csv(output_dir / 'event_study_results.csv', index=False)\n",
                "    print(f\"Saved results to {output_dir / 'event_study_results.csv'}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"EVENT STUDY ANALYSIS COMPLETE\")\n",
                "print(\"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
