{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02 - NLP Processing Pipeline\n",
                "\n",
                "This notebook processes budget speech PDFs through the NLP pipeline:\n",
                "1. Extract text from PDFs\n",
                "2. Tokenize into sentences\n",
                "3. Classify sectors (soft probabilities)\n",
                "4. Analyze sentiment\n",
                "5. Score certainty and actionability\n",
                "\n",
                "## Outputs\n",
                "- Processed sentences with sector probabilities\n",
                "- Sentiment scores\n",
                "- Importance weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "# Add src to path\n",
                "project_root = Path.cwd().parent\n",
                "sys.path.insert(0, str(project_root))\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Download NLTK data\n",
                "import nltk\n",
                "nltk.download('punkt', quiet=True)\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "nltk.download('stopwords', quiet=True)\n",
                "\n",
                "print(f\"Project root: {project_root}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load configuration\n",
                "import yaml\n",
                "\n",
                "config_dir = project_root / 'config'\n",
                "\n",
                "with open(config_dir / 'sectors.yaml', 'r', encoding='utf-8') as f:\n",
                "    sectors_config = yaml.safe_load(f)\n",
                "\n",
                "with open(config_dir / 'event_dates.yaml', 'r', encoding='utf-8') as f:\n",
                "    event_dates = yaml.safe_load(f)\n",
                "\n",
                "print(f\"Loaded {len(sectors_config['sectors'])} sector definitions\")\n",
                "print(f\"Loaded {len(event_dates['budget_events'])} budget events\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load and Extract Text from Budget Speech"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.nlp import extract_text_with_fallback, clean_text\n",
                "\n",
                "# Find available speech PDFs\n",
                "speech_files = list(project_root.glob('*.pdf'))\n",
                "print(f\"Found {len(speech_files)} budget speech PDFs:\")\n",
                "for f in sorted(speech_files):\n",
                "    print(f\"  - {f.name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Process the most recent budget (2024-25)\n",
                "target_speech = None\n",
                "for f in speech_files:\n",
                "    if '2024' in f.name or '202425' in f.name.replace('_', ''):\n",
                "        target_speech = f\n",
                "        break\n",
                "\n",
                "if target_speech is None:\n",
                "    target_speech = speech_files[-1]  # Use latest\n",
                "\n",
                "print(f\"Processing: {target_speech.name}\")\n",
                "\n",
                "# Extract text\n",
                "raw_text = extract_text_with_fallback(str(target_speech))\n",
                "print(f\"\\nExtracted {len(raw_text)} characters\")\n",
                "print(\"\\nFirst 1000 characters:\")\n",
                "print(raw_text[:1000])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clean the text\n",
                "cleaned_text = clean_text(raw_text)\n",
                "print(f\"Cleaned text: {len(cleaned_text)} characters\")\n",
                "print(\"\\nSample (first 1500 chars):\")\n",
                "print(cleaned_text[:1500])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Tokenize into Sentences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.nlp import tokenize_speech, estimate_timestamps, validate_sentences\n",
                "from datetime import datetime\n",
                "from src.utils.time_utils import IST\n",
                "\n",
                "# Tokenize\n",
                "sentences = tokenize_speech(cleaned_text)\n",
                "print(f\"Tokenized into {len(sentences)} sentences\")\n",
                "\n",
                "# Sample sentences\n",
                "print(\"\\nSample sentences:\")\n",
                "for i, s in enumerate(sentences[:5]):\n",
                "    print(f\"  [{i}] {s['text'][:100]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get budget timing from config\n",
                "fiscal_year = '2024-25'\n",
                "budget_info = event_dates['budget_events'].get(fiscal_year, {})\n",
                "\n",
                "print(f\"Budget {fiscal_year}:\")\n",
                "print(f\"  Date: {budget_info.get('date')}\")\n",
                "print(f\"  Speech: {budget_info.get('speech_start')} - {budget_info.get('speech_end')}\")\n",
                "\n",
                "# Create speech start time\n",
                "speech_date = budget_info.get('date', '2024-07-23')\n",
                "speech_start_time = budget_info.get('speech_start', '11:00')\n",
                "\n",
                "speech_start = datetime.strptime(f\"{speech_date} {speech_start_time}\", \"%Y-%m-%d %H:%M\")\n",
                "speech_start = IST.localize(speech_start)\n",
                "print(f\"  Speech start: {speech_start}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Estimate timestamps for each sentence\n",
                "from src.ingestion import get_speech_duration_minutes\n",
                "\n",
                "speech_duration = get_speech_duration_minutes(fiscal_year)\n",
                "print(f\"Speech duration: {speech_duration} minutes\")\n",
                "\n",
                "# Add timestamps\n",
                "sentences = estimate_timestamps(sentences, speech_start, speech_duration)\n",
                "\n",
                "# Validate\n",
                "sentences = validate_sentences(sentences)\n",
                "\n",
                "# Convert to DataFrame\n",
                "sentences_df = pd.DataFrame(sentences)\n",
                "print(f\"\\nSentences DataFrame shape: {sentences_df.shape}\")\n",
                "sentences_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Classify Sectors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.nlp import classify_sectors_batch\n",
                "\n",
                "# Get sector keywords from config\n",
                "sector_keywords = {}\n",
                "for sector_key, sector_info in sectors_config['sectors'].items():\n",
                "    sector_keywords[sector_key] = sector_info.get('keywords', [])\n",
                "\n",
                "print(f\"Loaded keywords for {len(sector_keywords)} sectors\")\n",
                "\n",
                "# Classify sentences\n",
                "prob_cols = classify_sectors_batch(sentences_df['text'].tolist(), sector_keywords)\n",
                "\n",
                "# Add to DataFrame\n",
                "for col, probs in prob_cols.items():\n",
                "    sentences_df[col] = probs\n",
                "\n",
                "print(f\"Added {len(prob_cols)} sector probability columns\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View sector probabilities\n",
                "prob_columns = [c for c in sentences_df.columns if c.startswith('prob_')]\n",
                "print(\"Sector probability statistics:\")\n",
                "sentences_df[prob_columns].describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find sentences with high sector relevance\n",
                "for sector in list(sector_keywords.keys())[:5]:\n",
                "    col = f'prob_{sector}'\n",
                "    if col in sentences_df.columns:\n",
                "        top_sentences = sentences_df.nlargest(3, col)[['position', col, 'text']]\n",
                "        print(f\"\\nTop sentences for {sector}:\")\n",
                "        for _, row in top_sentences.iterrows():\n",
                "            print(f\"  [{row[col]:.2f}] {row['text'][:80]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Sentiment Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.nlp import analyze_sentiment_batch, compute_fiscal_intensity\n",
                "\n",
                "# Analyze sentiment\n",
                "sentiments = analyze_sentiment_batch(sentences_df['text'].tolist())\n",
                "\n",
                "# Add to DataFrame\n",
                "for key in ['compound', 'positive', 'negative', 'neutral']:\n",
                "    sentences_df[f'sentiment_{key}'] = [s.get(key, 0) for s in sentiments]\n",
                "\n",
                "print(\"Sentiment statistics:\")\n",
                "sentences_df[['sentiment_compound', 'sentiment_positive', 'sentiment_negative']].describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fiscal intensity (monetary figures, percentages)\n",
                "fiscal_scores = compute_fiscal_intensity(sentences_df['text'].tolist())\n",
                "sentences_df['fiscal_intensity'] = [f.get('fiscal_intensity', 0) for f in fiscal_scores]\n",
                "\n",
                "print(f\"Fiscal intensity: mean={sentences_df['fiscal_intensity'].mean():.3f}\")\n",
                "\n",
                "# Show high fiscal intensity sentences\n",
                "print(\"\\nHigh fiscal intensity sentences:\")\n",
                "for _, row in sentences_df.nlargest(5, 'fiscal_intensity').iterrows():\n",
                "    print(f\"  [{row['fiscal_intensity']:.2f}] {row['text'][:100]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize sentiment over speech\n",
                "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
                "\n",
                "# Sentiment compound\n",
                "ax1 = axes[0]\n",
                "ax1.plot(sentences_df['position'], sentences_df['sentiment_compound'], alpha=0.7)\n",
                "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
                "ax1.set_ylabel('Sentiment (Compound)')\n",
                "ax1.set_title('Sentiment Over Speech')\n",
                "\n",
                "# Rolling average\n",
                "rolling_sentiment = sentences_df['sentiment_compound'].rolling(window=10, center=True).mean()\n",
                "ax1.plot(sentences_df['position'], rolling_sentiment, color='red', linewidth=2, label='10-sentence MA')\n",
                "ax1.legend()\n",
                "\n",
                "# Fiscal intensity\n",
                "ax2 = axes[1]\n",
                "ax2.bar(sentences_df['position'], sentences_df['fiscal_intensity'], alpha=0.7, color='green')\n",
                "ax2.set_xlabel('Sentence Position')\n",
                "ax2.set_ylabel('Fiscal Intensity')\n",
                "ax2.set_title('Fiscal Intensity Over Speech')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Certainty and Actionability Scoring"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.nlp import score_certainty_batch, score_actionability_batch\n",
                "\n",
                "# Score certainty\n",
                "sentences_df['certainty_score'] = score_certainty_batch(sentences_df['text'].tolist())\n",
                "\n",
                "# Score actionability\n",
                "sentences_df['actionability_score'] = score_actionability_batch(sentences_df['text'].tolist())\n",
                "\n",
                "print(\"Certainty and Actionability:\")\n",
                "print(sentences_df[['certainty_score', 'actionability_score']].describe())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate importance weight\n",
                "sentences_df['importance_weight'] = (\n",
                "    0.3 * sentences_df['certainty_score'] +\n",
                "    0.4 * sentences_df['actionability_score'] +\n",
                "    0.3 * sentences_df['fiscal_intensity']\n",
                ")\n",
                "\n",
                "print(\"\\nMost important sentences:\")\n",
                "for _, row in sentences_df.nlargest(5, 'importance_weight').iterrows():\n",
                "    print(f\"\\n[Weight: {row['importance_weight']:.2f}]\")\n",
                "    print(f\"  Certainty: {row['certainty_score']:.2f}, Actionability: {row['actionability_score']:.2f}\")\n",
                "    print(f\"  Text: {row['text'][:150]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add metadata\n",
                "sentences_df['fiscal_year'] = fiscal_year\n",
                "sentences_df['budget_date'] = budget_info.get('date')\n",
                "\n",
                "# Save to intermediate directory\n",
                "output_dir = project_root / 'data' / 'intermediate' / 'speech_text'\n",
                "output_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "output_path = output_dir / f'{fiscal_year.replace(\"-\", \"_\")}_sentences.parquet'\n",
                "sentences_df.to_parquet(output_path)\n",
                "\n",
                "print(f\"Saved processed sentences to {output_path}\")\n",
                "print(f\"  Shape: {sentences_df.shape}\")\n",
                "print(f\"  Columns: {list(sentences_df.columns)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Also save as CSV for easy viewing\n",
                "csv_path = output_dir / f'{fiscal_year.replace(\"-\", \"_\")}_sentences.csv'\n",
                "sentences_df.to_csv(csv_path, index=False)\n",
                "print(f\"Also saved as CSV: {csv_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sector attention distribution\n",
                "prob_columns = [c for c in sentences_df.columns if c.startswith('prob_')]\n",
                "\n",
                "# Calculate total attention per sector\n",
                "sector_attention = {}\n",
                "for col in prob_columns:\n",
                "    sector = col.replace('prob_', '')\n",
                "    # Weight by importance\n",
                "    weighted_attention = (sentences_df[col] * sentences_df['importance_weight']).sum()\n",
                "    sector_attention[sector] = weighted_attention\n",
                "\n",
                "attention_df = pd.DataFrame([\n",
                "    {'sector': k, 'attention': v} for k, v in sector_attention.items()\n",
                "]).sort_values('attention', ascending=True)\n",
                "\n",
                "# Plot\n",
                "fig, ax = plt.subplots(figsize=(12, 8))\n",
                "ax.barh(attention_df['sector'], attention_df['attention'], color='steelblue')\n",
                "ax.set_xlabel('Weighted Attention Score')\n",
                "ax.set_title(f'Sector Attention in Budget {fiscal_year}')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"NLP PROCESSING COMPLETE\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nProcessed Budget: {fiscal_year}\")\n",
                "print(f\"Total sentences: {len(sentences_df)}\")\n",
                "print(f\"Sectors classified: {len(prob_columns)}\")\n",
                "print(f\"\\nTop 5 Sectors by Attention:\")\n",
                "for _, row in attention_df.tail(5).iterrows():\n",
                "    print(f\"  {row['sector']}: {row['attention']:.2f}\")\n",
                "print(f\"\\nOutput saved to: {output_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}